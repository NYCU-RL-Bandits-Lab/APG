action_num: 3
env: Random MDP
eta: 0.4
fname: test
gamma: 0.9
initial_state_distribution_dict:
  s1: 0.21
  s2: 0.21
  s3: 0.58
initial_theta_dict:
  s1:
  - 2
  - 1
  - 0
  s2:
  - 5
  - 3
  - 5
  s3:
  - 5
  - 0
  - 4
log_root: ./logs/assum-4
random_mdp: true
reward_dict:
  s1_a1: 0.29
  s1_a2: 0.98
  s1_a3: 0.88
  s2_a1: 0.06
  s2_a2: 0.29
  s2_a3: 0.06
  s3_a1: 0.52
  s3_a2: 0.12
  s3_a3: 0.67
run_algos: []
seed_num: 3
state_action_num:
- 3
- 3
state_num: 3
stochastic: false
transition_prob_dict:
  s1a1_s1: 0.06
  s1a1_s2: 0.47
  s1a1_s3: 0.47
  s1a2_s1: 0.84
  s1a2_s2: 0.04
  s1a2_s3: 0.12
  s1a3_s1: 0.16
  s1a3_s2: 0.42
  s1a3_s3: 0.42
  s2a1_s1: 0.04
  s2a1_s2: 0.84
  s2a1_s3: 0.12
  s2a2_s1: 0.78
  s2a2_s2: 0.11
  s2a2_s3: 0.11
  s2a3_s1: 0.16
  s2a3_s2: 0.42
  s2a3_s3: 0.42
  s3a1_s1: 0.42
  s3a1_s2: 0.42
  s3a1_s3: 0.16
  s3a2_s1: 0.12
  s3a2_s2: 0.84
  s3a2_s3: 0.04
  s3a3_s1: 0.24
  s3a3_s2: 0.09
  s3a3_s3: 0.67
