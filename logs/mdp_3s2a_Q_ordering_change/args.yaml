APG_epoch_size: 50000
APG_graphing_size:
- 50000
PG_epoch_size: 1000
PG_graphing_size:
- 1000
V_opt: 3.3210332103321045
V_opts:
- 3.3210332103321045
- 3.690036900369005
- 3.490036900369005
- 2.9889298892988942
action_num: 2
chunk_size: 100000
d_rho_opt:
- 0.3690036900369004
- 0.3321033210332104
- 0.0
- 0.29889298892988936
env: ./mdp_env/mdp_3s2a_Q_ordering_change.yaml
eta: 0.00012499999999999992
fname: mdp_3s2a_Q_ordering_change
gamma: 0.9
initial_state_distribution_dict:
  s1: 1.0
  s2: 0.0
  s3: 0.0
  s4: 0.0
initial_theta_dict:
  s1:
  - 0.0
  - 0.0
  s2:
  - 0.0
  - 2.0
  s3:
  - 0.0
  - -inf
  s4:
  - 0.0
  - -inf
log_root: ./logs
max_iter: 100000000
optimal_policy:
- 0
- 0
- 0
- 0
random_mdp: false
reward_dict:
  s1_a1: 0.0
  s1_a2: 0.0
  s2_a1: 1.0
  s2_a2: 0.0
  s3_a1: 0.8
  s3_a2: 0.0
  s4_a1: 0.0
  s4_a2: 0.0
run_algos:
- APG
seed_num: 1
state_action_num:
- null
- null
state_num: 4
stochastic: false
transition_prob_dict:
  s1a1_s1: 0.0
  s1a1_s2: 1.0
  s1a1_s3: 0.0
  s1a1_s4: 0.0
  s1a2_s1: 0.0
  s1a2_s2: 0.0
  s1a2_s3: 1.0
  s1a2_s4: 0.0
  s2a1_s1: 0.0
  s2a1_s2: 0.0
  s2a1_s3: 0.0
  s2a1_s4: 1.0
  s2a2_s1: 0.0
  s2a2_s2: 0.0
  s2a2_s3: 0.0
  s2a2_s4: 1.0
  s3a1_s1: 0.0
  s3a1_s2: 0.0
  s3a1_s3: 0.0
  s3a1_s4: 1.0
  s3a2_s1: 0.0
  s3a2_s2: 0.0
  s3a2_s3: 0.0
  s3a2_s4: 1.0
  s4a1_s1: 1.0
  s4a1_s2: 0.0
  s4a1_s3: 0.0
  s4a1_s4: 0.0
  s4a2_s1: 1.0
  s4a2_s2: 0.0
  s4a2_s3: 0.0
  s4a2_s4: 0.0
